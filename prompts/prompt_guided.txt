You are solving a multi-armed bandit decision-making problem for 25 rounds.

There are {n_actions} arms (numbered 0 to {n_actions-1}). At each round, you need to choose one arm to pull.

After pulling an arm, you will receive a reward of either 1 (win) or 0 (loss). The reward is determined by the arm's true probability of winning, which is fixed but unknown.

Here is the detailed history of arm pulls and outcomes:
{history}

For each arm, you have access to:
- Total number of pulls
- Cumulative win count
- Cumulative loss count
- Win rate (wins/total pulls)
- Cumulative reward

Your goal is to maximize your total reward over time. Let's think step by step:

1. Early in the game (first 5-8 rounds):
   - Try different arms to learn their performance
   - Focus on exploration to gather information
   - Don't commit to any arm too quickly

2. Middle of the game (rounds 9-15):
   - Start balancing exploration and exploitation
   - Pay more attention to arms with good win rates
   - Still maintain some exploration of less tried arms

3. Later in the game (rounds 16-25):
   - Focus more on exploitation
   - Choose arms with proven good performance
   - Only explore if there's significant uncertainty

When making your decision, consider:
- Arms with high win rates but few pulls (potential for good performance)
- Arms with consistent performance over many pulls (reliable choices)
- Arms that haven't been tried enough (exploration opportunity)
- The trade-off between exploration and exploitation based on the current round

Please respond with ONLY the arm number (0-{n_actions-1}) and a detailed explanation of your choice, including:
- Which phase of the game you're in
- Why you chose this arm
- How your choice balances exploration and exploitation

Your response must be in the format: 'N # explanation' where N is the arm number (0-{n_actions-1}) 